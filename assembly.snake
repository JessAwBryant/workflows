"""
Workflow for assembling a single metagenoic sample

 * renames reads to sample name
 * cleans with bbduk
 * runs assembler (SPAdes)
 * annotates (rRNAs, genes)

####
# Required conda packages:
#
# snakemake bbmap infernal spades prodigal pandas biopython bfc (optional)
###

"""
import os, re, sys, glob, yaml, pandas, subprocess, tempfile
from Bio import SeqIO, __version__ as BIOPYTHON_VERSION

include: "common/common.snake"
include: "common/mapping_bwa.snake"

try:
    include: "assembly/{assembler}.snake".format(
                assembler=config.get('assembler', 'spades'))
except:
    raise Excption("Unknown assembler: " + config['assembler'])

# Some hoops to jump through because the contigs stats aren't a proper script
# yet
snakefile_path=os.path.dirname(os.path.abspath(workflow.snakefile))
config['pymg_dir'] = os.path.join(snakefile_path, 'tools', 'pymg')
sys.path.append(config['pymg_dir'])
from edl import __version__ as PYMG_VERSION

include: "annotation/cmsearch.snake"
include: "qc/bfc.s
if 'models' not in config['system_paths']:
    config['system_paths']['models']=os.path.join(snakefile_path,'models')
if 'bbmap' not in config['system_paths']:
    try:
        cmd='find `which bbduk.sh | sed -r s/bbduk\.sh/../` ' + \
             '-name "adapters.fa" | sed -r s#/resources/adapters.fa##'
        config['system_paths']['bbmap'] = subprocess.check_output(cmd,
                                                                  shell=True)\
                                                    .decode().strip()
    except:
        print("Can't find bbmap resources, please set system_paths->bbmap in the snakemake config file")

# available rRNA DBs
rrna_db_dot_fmts=[]
for db, db_data in config['dbs'].items():
    if db_data.get('type')=='rrna':
        fmt = db_data.get('format','lastdb')
        rrna_db_dot_fmts.append("{}.{}".format(db,fmt))

# Define some global variables
read_directions = ['R1','R2']

# which tool for error correction
if 'bfc' in config:
    # use BFC if configured and nothing else pecified
    config.setdefault('error_correction','bfc')
error_correction = config.get('error_correction','spades').lower()

# default configuration
defaults={'spades':{'ram':255,
                    'threads':20,
                    'tmp_dir':'.',
                    'kmers':'21,33,55,77,99,127'},
          'cmsearch':{'threads':8},
          'bbduk':{'ram':'1g',
                   'adapter_flags':'ktrim=r k=23 mink=11 hdist=1 tbo tpe tbo tpe',
                   'phix_flags':'k=27 hdist=1 qtrim=rl trimq=17 cardinality=t mingc=0.05 maxgc=0.95'},
          'bfc':{'ram':'5g',
                 'threads':20,
                 'params':'-1 -k 21'}}

for param, pdefaults in defaults.items():
    for key, value in pdefaults.items():
        config.setdefault(param,{}).setdefault(key,value)

#######
# Functions
def get_version(command, version_flag='--version', 
                cmd_prefix='',
                lines=None,
                regular_expression=None):
    """
    Gets the version string from a command

    cmd_prefix is useful if you need an interpreter (bash, python, etc)
    
    lines can be a line number (int), a slice object, or an iterable indicating which lines of the output to grab

    if regular_expression given, the first captured group is returned.
    """
    out = subprocess.check_output(" ".join([cmd_prefix,
                                            command,
                                            version_flag,
                                            "; exit 0"]),
                                  stderr=subprocess.STDOUT,
                                  shell=True).decode()

    # select specific lines
    if lines is not None:
        out_lines = out.split("\n")
        if isinstance(lines,slice):
            out = "\n".join(out_lines[lines])
        elif isinstance(lines, int):
            out = out_lines[lines]
        else:
            out = "\n".join(out_lines[i] for i in lines)

    # apply regular expression if given
    if regular_expression is None:
        return out.strip()
    else:
        return regular_expression.search(out).group(1)

def get_model_version(molecule, models_dir):
    """
    Gets the version of the cm and hmm models used from a "VERSION"
    file in the models folder.

    Returns "Unknown" if the version file is missing.
    """

    # Look for a VERSION file
    version_file = os.path.join(models_dir,'VERSION')
    if os.path.exists(version_file):
        with open(version_file) as VF:
            return next(VF).strip()

    # Fall back to README file
    readme_file = os.path.join(models_dir,'README') 
    if os.path.exists(readme_file):
        with open(readme_file) as RMF:
            for line in RMF:
                if re.search('r(Release\s*[0-9.]+)',line):
                    return "RFAM " + line.strip()

    # Fallback to unknown
    return "Unknown"

def parse_stats(stats_file):
    """
    pull out the read and base counts from a prinseq output file.
    Returns a two item dict with integer values and keys: 'reads', 'bases'
    """
    stats = pandas.read_table(stats_file,names=('module','key','value'),index_col=1)['value']
    return {k:int(stats[k]) for k in ['reads','bases']}

####
# Flexible all target
#
all_targets = [
        "contigs.report",
        "contigs.stats.txt",
        "contigs.annotations.gff",
        "contigs.annotations.coverage.tsv",
        "contigs.annotations.faa.stats",
        #"contigs.annotations.coverage.tsv",
        "mapping/reads.vs.contigs.bam.bai"
    ]

start = config.get("start","raw")
if start == "raw":
    read_types=['raw','noadapt','cleaned']
elif start == 'cleaned':
    read_types = ['cleaned']
elif start == 'corrected':
    read_types = ['corrected.{}'.format(error_correction)]
else:
    read_types = []
if 'cleaned' in read_types and error_correction!='spades':
    read_types.append('corrected.{}'.format(error_correction))

############
# RULES
#
# Start with the final product(s):
rule all:
    input:
        all_targets
    log:
        "logs/all.log"

rule rename_raw_reads:
    """ Rename reads to be based on a "sample" name """
    input:
        lambda wildcards: config['inputs'][wildcards.sample][wildcards.direction]
    output:
        temp("{sample}/{direction}.renamed.fastq")
    log:
        "logs/{sample}_{direction}.renamed.fastq.log"
    benchmark:
        "benchmarks/{sample}/rename_reads_{direction}.txt"
    version:
        get_version('miraconvert','-v')
    shell:
        "miraconvert -f fastq -R {wildcards.sample} -S solexa -t fastq {input} {output} > {log}"

rule interleave:
    """
    Interleave forward and reverse reads while reanaming
    """
    input:
        expand("{{sample}}/{direction}.renamed.fastq", \
                                    direction=read_directions),
    output:
        "{sample}/reads.raw.fastq"
    log:
        "logs/{sample}_reads.raw.fastq.log"
    benchmark:
        "benchmarks/{sample}/interleave.txt"
    version:
        get_version('seqtk','',
                    regular_expression=re.compile(r'Version:\s*(\S+)'))
    shell:
        "seqtk mergepe {input} > {output}"

rule merge_interleaved:
    """
    Combine all interleaved raw reads files into one for no-trim assembly
    """
    input:
        expand("{sample}/reads.raw.fastq", sample=config['inputs'])
    output:
        "reads.raw.fastq.gz"
    log:
        "logs/reads.raw.fastq.log"
    benchmark:
        "benchmarks/merge_interleave.txt"
    shell:
        "cat {input} | gzip -c > {output}"

rule trim_bbduk_adapters:
    """
    remove adapters with bbduk
    """
    input:
        "reads.raw.fastq.gz"
    output:
        temp("reads.noadapt.fastq.gz")
    benchmark:
        "benchmarks/reads.noadapt.fastq.txt"
    log:
        "logs/reads.noadapt.log"
    version:
        get_version('bbduk.sh',
                    lines=[1,],)
    params:
        adapter_file=config['system_paths']['bbmap'] \
                        + "/resources/adapters.fa"
    shell:
        "bbduk.sh -Xmx{config[bbduk][ram]} \
          in={input} out={output} \
          interleaved overwrite {config[bbduk][adapter_flags]} \
          ref={params.adapter_file} 2> {log}"

rule trim_bbduk_phix:
    """
    remove phiX with bbduk
    """
    input:
        "reads.noadapt.fastq.gz"
    output:
        "reads.cleaned.fastq.gz",
        "reads.cleaned.gchist.txt"
    benchmark:
        "benchmarks/reads.cleaned.fastq.txt"
    log:
        "logs/reads.cleaned.log"
    version:
        get_version('bbduk.sh',
                    lines=[1,],)
    params:
        phiX_file=config['system_paths']['bbmap'] \
                         + "/resources/phix174_ill.ref.fa.gz"
    shell:
        "bbduk.sh -Xmx{config[bbduk][ram]} \
          in={input} out={output[0]} gchist={output[1]} \
          interleaved overwrite {config[bbduk][phix_flags]} \
          ref={params.phiX_file} 2> {log}"

rule bfc_error_correction:
    """
    Use BFC to correct errors
    """
    input:
        "reads.cleaned.fastq.gz"
    output:
        "reads.corrected.bfc.fastq.gz"
    benchmark:
        "benchmarks/reads.kmer_trimmed.txt"
    log:
        "logs/reads.kmer_trimmed.log"
    version:
        get_version('bfc','-v')
    threads:
        int(config['bfc']['threads'])
    shell:
        "bfc -s {config[bfc][ram]} {config[bfc][params]} \
          -t {threads} {input} 2> {log} \
          | seqtk dropse - \
          | gzip -c > {output}"

rule run_spades:
    """
    $(CONTIGS_LOCAL): $(R1_FASTQ) $(R2_FASTQ) | $(OUTPUT_DIR)
        $(SPADES) -m $(SPADES_RAM) -o $(OUTPUT_DIR)/spades --pe1-1 $(R1_FASTQ) --   pe1-2 $(R2_FASTQ) --only-assembler --meta
    """
    input:
        "reads.corrected.{}.fastq.gz".format(error_correction) \
            if error_correction!='spades' \
            else "reads.cleaned.fastq.gz"
    output:
        "spades/contigs.fasta"
    log:
        "logs/spades.log"
    threads:
        int(config['spades']['threads'])
    params:
        ram_limit=config['spades']['ram'],
        kmers=config['spades']['kmers'],
        tmp_dir=tempfile.TemporaryDirectory( \
                                suffix='tmp', \
                                prefix='spades', \
                                dir=config['spades']['tmp_dir']),
        continue_flags="--continue" if "continue_spades" in config else "",
        skip_ec='--only-assembler' if error_correction!='spades' else ""
    benchmark:
        "benchmarks/spades.txt"
    version:
        get_version('spades.py')
    shell:
        """
        spades.py -m {params.ram_limit} -o spades -t {threads} \
                  -k {params.kmers} --tmp-dir {params.tmp_dir.name} \
                  --pe1-12 {input} \
                  {params.continue_flags} {params.skip_ec} --meta 
        """

rule contigs_fasta:
    """
    Rename contigs
    """
    input:
        "spades/contigs.fasta"
    output:
        "contigs.fasta"
    log:
        "logs/contigs.fasta.link.log"
    benchmark:
        "benchmarks/contigs.fasta.txt"
    version:
        "biopython-{}".format(BIOPYTHON_VERSION)
    run:
        root_name = config['assembly_name']
        with open(input[0]) as INPUT:
            with open(output[0], 'w') as OUTPUT:
                for i, (title, sequence) \
                    in enumerate(SeqIO.FastaIO.SimpleFastaParser(INPUT)):

                    contig_name = "%s_c%d" % (root_name, i+1)
                    OUTPUT.write(">%s %s\n%s\n" % (contig_name,
                                                   title,
                                                   sequence))

rule cmsearch:
    """ 
    Find likely rRNA genes using cmsearch and RFAM models
    """
    input:
        "contigs.fasta"
    output:
        temp("contigs.annotations.{molecule}.tbl")
    params:
        cmfile=os.path.join(config['system_paths']['models'],'{molecule}.cm'),
        flags=lambda wildcards:\
          "--hmmonly" if wildcards.molecule=='rRNA' else ""
    benchmark:
        "benchmarks/contigs.annotations.{molecule}.tbl.txt"
    threads:
        int(config['cmsearch']['threads'])
    version:
        lambda wildcards:\
            get_version('cmsearch',
                        '-h',
                        regular_expression=re.\
                          compile(r'^#\s*(INFERNAL[\s_-.]*[0-9.]+.+$')) \
                + " :: " + get_model_version(wildcards.molecule,
                                             config['system_paths']['models'])
    shell:
        "cmsearch {params.flags} -o /dev/null --tblout {output} --cpu {threads} {params.cmfile} {input}"

rule cmsearch_tbl_to_gff:
    """
    convert cmsearch output to gff
    """
    input:
        "contigs.annotations.{molecule}.tbl"
    output:
        "contigs.annotations.{molecule,[^.]+}.gff"
    benchmark:
        "benchmarks/contigs.annotations.{molecule}.gff.txt"
    version:
        get_version('filter_blast_m8.py')
    shell:
        "sort {input} | filter_blast_m8.py -v -f cmsearch --gff | sort | filter_blast_m8.py -f gff -s score > {output}"

rule prodigal:
    """ get gene predictions form prodigal """
    input:
        "contigs.fasta"
    output:
        "contigs.annotations.genes.gff"
    benchmark:
        "benchmarks/contigs.annotations.genes.txt"
    version:
        get_version('prodigal','-v')
    shell:
        "prodigal -i {input} -f gff -o {output} -p meta -q"

rule merge_annotations:
    """ 
    merge all GFFs and drop overlaps 
    
    For now, we drop everything that overlaps, but I will reate a script to drop genes only if they overlap rRNA
    """
    input:
        contigs="contigs.fasta",
        rrnas="contigs.annotations.rRNA.gff",
        trnas="contigs.annotations.tRNA.gff",
        genes="contigs.annotations.genes.gff"
    output:
        "contigs.annotations.gff",
        "contigs.annotations.fna",
        "contigs.annotations.faa"
    params:
        output_root="contigs.annotations"
    benchmark:
        "benchmarks/contigs.annotations.merge.txt"
    version:
        get_version('merge_gffs.py')
    shell:
        "merge_gffs.py -r {input.rrnas} -r {input.trnas} -c {input.genes} {input.contigs} {params.output_root}"

rule map_reads:
    """
    map cleaned reads onto contigs
    """
    input:
        "mapping/contigs.index.bwa.bwt",
        "reads.corrected.{}.fastq".format(error_correction) \
            if error_correction!='spades' \
            else "reads.cleaned.fastq"
    output:
        temp("mapping/reads.vs.contigs.sam")
    log:
        "logs/bwa_mem.log"
    benchmark:
        "benchmarks/bwa_mem_sam.txt"
    version:
        get_version('bwa', 
                    version_flag="", 
                    regular_expression=re.compile(r'Version:\s*(\S[^\n\r]+\S)'))
    threads:
        20
    shell:
        "bwa mem -t {threads} mapping/contigs.index.bwa {input[1]} 2> {log}  > {output}"

rule contig_mapped_read_counts:
    """ 
    Count number of reads mapped to each contig. 

    This just pulls out the refernce column (3) from the SAM output
    and uses uniq -c to get a count.
    """
    input:
        "mapping/reads.vs.contigs.bam.bai"
    output:
        "mapping/reads.vs.contigs.read_counts"
    benchmark:
        "benchmarks/bwa_count_reads.txt"
    version:
        get_version('samtools', lines=0)
    params:
        input="mapping/reads.vs.contigs.bam"
    shell:
        "samtools view {params.input} \
         | cut -f 3 \
         | uniq -c \
         > {output}"

rule contig_mapped_read_depths:
    """
    Get read depth at each base in each contig. This is needed for
    coverage calculation later.
    """
    input:
        "mapping/reads.vs.contigs.bam.bai"
    output:
        "mapping/reads.vs.contigs.depths"
    benchmark:
        "benchmarks/bwa_get_depths.txt"
    version:
        get_version('samtools', lines=0)
    params:
        input="mapping/reads.vs.contigs.bam"
    shell:
        "samtools depth {params.input} > {output}"

rule contig_stats_table:
    """
    Combine stats from fasta header with depths and read counts to 
    make a master table of contig stats
    """
    input:
        "contigs.fasta",
        rules.contig_mapped_read_depths.output,
        rules.contig_mapped_read_counts.output
    output:
        "contigs.stats.txt",
        "contigs.fasta.histograms"
    benchmark:
        "benchmarks/contig.stats.txt"
    version:
        "py-metagenomics-{}".format(PYMG_VERSION)
    shell:
        "python {config[system_paths][pymg_dir]}/edl/assembly.py get_contig_stats {input} {output} txt_width=75 log=True"

rule long_rrna_gff:
    """ find long LSU or SSU annotations """
    input:
        "{prefix}.gff"
    output:
        "{prefix}.{mol,[LS]SU}.gt{length,\d+}.gff"
    benchmark:
        "benchmarks/{prefix}.{mol}.gt{length}.time"
    version:
        get_version("filter_blast_m8")
    shell:
        "grep {wildcards.mol} {input} \
          | sort \
          | filter_blast_m8.py -s score -L {wildcards.length} -f gff \
            --nonoverlapping \
          > {output}"

rule rrna_fasta:
    """ extract rRNA fasta """
    input:
        contigs="contigs.fasta",
        gff="{prefix}.gff"
    output:
        "{prefix}.gff.fna"
    benchmark:
        "benchmarks/{prefix}.gff.fna.time"
    version:
        get_version("get_sequences_from_m8.py")
    shell:
        "cat {input.contigs} \
            | get_sequences_from_m8.py -f gff {input.gff} \
            > {output}"

rule map_genes_to_bwadb:
    """
    map genes to BWA db. 
    There must be a bwa formatted db path configured in:
        config['dbs'][wildcards.db]['path']
    """
    input:
        "{file_root}.fna"
    output:
        ("{file_root}.fna.vs.{db}.sam")
    log:
        "logs/{file_root}.vs.{db}.sam.log"
    benchmark:
        "benchmarks/{file_root}.vs.{db}.sam.time"
    version:
        get_version('bwa', 
                    version_flag="", 
                    regular_expression=re.compile(r'Version:\s*(\S[^\n\r]+\S)'))
    threads:
        lambda w: config.get('threads',{}).get('bwa',20)
    params:
        db_path=lambda w: config['dbs'][w.db]['path'],
    shell:
        "bwa mem -t {threads} {params.db_path} {input} 2> {log}  > {output}"

rule rrna_report:
    """
    Look at the rRNA annotations and get table of nearly full length LSUs or SSUs
    """
    input:
        gff="contigs.annotations.rRNA.{mol}.gt{length}.gff",
        hits=lambda w: expand("contigs.annotations.rRNA.{mol}.gt{length}" + \
                              ".gff.fna.vs.{db_dot_fmt}",
                              db_dot_fmt=[s for s in rrna_db_dot_fmts \
                                          if re.search(w.mol,s) is not None],
                              mol=w.mol,
                              length=w.length,
                    ),
        stats="contigs.stats.txt"
    output:
        "contigs.annotations.rRNA.{mol,[LS]SU}.gt{length,\d+}.tsv"
    params:
        length=1200
    run:
        # get map from contigs to coverage
        coverages = pandas.read_table(input.stats,
                                      index_col=0,
                                      usecols=['contig','md cov',])['md cov']
        # table of rRNA model hits from filtered GFF
        features = pandas.read_table(input.gff,
                                     header=None,
                                     names=['contig',
                                            'tool',
                                            'type',
                                            'start',
                                            'end',
                                            'score',
                                            'notes'],
                                     usecols=[0,1,2,3,4,5,8])
        # add coverages
        features['coverage'] = [coverages[features.loc[i,'contig']]\
                                for i in features.index]
        # index on contig, start, and end, and just take a few columns
        output_table = features.set_index(['contig', 'start', 'end'])\
                                            [['coverage', 'type', 'score']]
        # get Silva annotation(s) for each feature
        from edl.blastm8 import generate_hits
        from edl.util import parseMapFile
        for hit_table in input.hits:
            hit_table_name = re.search(r'gff\.fna\.vs\.(.+)', hit_table).group(1)
            db, format = hit_table_name.split('.',1)
            id_name_file = config['dbs'][db]['path'] + ".ids"
            hit_descriptions = parseMapFile(id_name_file)
            for gene, hits in generate_hits(hit_table, format=format):
                # take the first hit
                hit = list(hits)[0]
                contig, start, end = re.search(r'^(.+)_(\d+)_(\d+)$', gene)\
                                                                        .groups()
                index = (contig, int(start), int(end))
                if index not in output_table.index:
                    index = (contig, int(end), int(start))
                desc = hit_descriptions[hit.hit]
                output_table.loc[index, db] = desc
        
        output_table.to_csv(output[0], sep='\t')

rule final_report:
    input:
        raw_read_stats=expand("{sample}/reads.raw.fastq.stats",sample=config['inputs'].keys()),
        read_stats=expand("reads.{read_type}.fastq.gz.stats", 
               read_type=read_types),
        contig_stats="contigs.fasta.stats",
        gene_stats="contigs.annotations.faa.stats",
        ssu_rrnas="contigs.annotations.rRNA.SSU.gt1200.tsv",
        lsu_rrnas="contigs.annotations.rRNA.LSU.gt2000.tsv",
        histograms='contigs.fasta.histograms'
    output:
        "contigs.report"
    run:
        read_stats={}

        # paired reads
        for i in input.raw_read_stats + input.read_stats:
            m = re.search(r'reads\.([a-z]+)\.fastq\.(?:gz.)?stats',i)
            if m is None:
                continue
            type=m.group(1)
            stats = parse_stats(i)

            # get sample name if this is from a sample
            m = re.match(r'^(.+)/reads',i)
            if m:
                type = m.group(1) + " " + type
            
            read_stats['{0} reads'.format(type)]=stats['reads']
            read_stats['{0} bases'.format(type)]=stats['bases']

        # contigs
        contig_stats={}
        stats = parse_stats(input.contig_stats)
        contig_stats['contigs']=stats['reads']
        contig_stats['contig bases']=stats['bases']

        # N##
        stats=pandas.read_table(input.histograms,sep=':',index_col='key', names=('key','value'),nrows=8)
        stats=stats.set_index(stats.index.str.strip())['value']
        contig_stats['N50']=int(stats['N50'])
        contig_stats['N75']=int(stats['N75'])
        contig_stats['N90']=int(stats['N90'])

        # annotations
        stats = parse_stats(input.gene_stats)
        contig_stats['genes']=stats['reads']

        # full length SSUs
        ssu_count=-1
        with open(input.ssu_rrnas) as INF:
            for line in INF:
                ssu_count+=1
        contig_stats['SSUs']=ssu_count

        # full length LSUs
        lsu_count=-1
        with open(input.lsu_rrnas) as INF:
            for line in INF:
                lsu_count+=1
        contig_stats['LSUs']=lsu_count

        report={"reads":read_stats,"assembly":contig_stats}
        with open(output[0],'w') as OUTF:
            OUTF.write(yaml.dump(report, default_flow_style=False))

rule annotation_coverage:
    """
    For each anntation, pull out the median coverage for its contig
    into a simple two column table
    """
    input:
        gff="contigs.annotations.gff",
        stats="contigs.stats.txt"
    output:
        table="contigs.annotations.coverage.tsv",
    run:
        # Read stats table, but just keep median coverage
        cov_dict = pandas.read_table(input.stats, index_col=0, usecols=['contig','md cov'])['md cov']
        with open(output.table,'w') as OUT:
            with open(input.gff) as GFF:
                feature_count = 0
                current_contig = 0
                for line in GFF:
                    contig = line.split()[0].strip()
                    if contig != current_contig:
                        feature_count=1
                        current_contig = contig
                    else:
                        feature_count+=1
                    OUT.write("{contig}_{feature_count}\t{coverage}\n"\
                               .format(contig=contig,
                                       feature_count=feature_count,
                                       coverage=cov_dict[contig]))




rule clean:
    params:
        samples=" ".join(config['inputs'])
    shell:
        "rm -rf logs benchmarks spades contigs.* reads.* {params.samples}"

# TODO?: apply contig filter (min length, min read count)
